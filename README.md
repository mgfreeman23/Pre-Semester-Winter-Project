# Pre-Semester-Winter-Project
Project for Center for AI in Society Club at USC

#### Malina Freeman 
#### mgfreema@usc.edu

This project involved using a neural network to classify news headlines as sarcastic or not sarcastic. This was an NLP Binary Classification task, and I approached it the same way we approached L4: by fine-tuning the pretrained BERT model.

##### Dataset:
There were two datasets linked to this project on Kaggle. One contained a ratio of about 1.3 non-sarcastic headline samples to sarcastic headline samples, while the other contained a respective ratio of 1.1. I decided to stick with the 1.3 ratio, since in practice there most likely exists more non-sarcasitc news headlines than sarcastic ones. Therefore, it's more practical to train our model on data that is representative of the real world. I processed the data by reading it into a pandas dataframe, ensuring there weren't any null values, and clearly comparing the number of data samples in each class with a bar graph. This will allows outside users to clearly see that the data is valid to be used and has significant representation for each class. I then extracted the headlines and sarcastic labels as lists, and I used tokenizer.encode_plus to get the data into a format suitable for the BERT model.

##### Model Development and Training:
I implemented a pretrained transformer architecture (BERT). Otherwise, this could've taken much longer to train and required more computing power. After getting my data into a good format for BERT, I split it into 80% training data, 10% validation data, and 10% testing data because this is roughly standard. I used the highest percentage for training data, since I knew my dataset was large enough that even 10% would still be thousands of headlines. Therefore, having the most training data possible would help the model perform better. 
I used the hyperparameters recommended from the BERT paper for fine-tuning BERT. Using the AdamW optimizer, I experimented with different initial learning rates. I set a learning rate scheduler to adjust this rate as training progressed. The initial learning rate that yielded the best results was a rate of 5e-5, the highest of the recommended rates. This could be due to how lower rates were taking too long to converge on minima or got stuck in local minima. I started training with 2 epochs, but training with 3 epochs resulted in better performance metrics for accuracy and MCC. Finally, I settled with a batch size of 32 because this performed slightly better than training with a batch size of 16. Larger batch sizes typically require fewer epochs to converge than smaller batch sizes. 
I added the linear sequence Classification layer on top of BERT, then trained and validated it with the headline and label data. I printed out time steps and MCC/Accuracy scores for each epoch of validation so I could see how my model was performing and try to adjust hyperparameters accordingly.

##### Model Evaluation/Results:
I chose to use MCC score and accuracy as the primary metrics for evaluating the model. The final test results were an MCC score of 0.870 and an accuracy of 0.935. I included accuracy because it is one of the easiest metrics to interpret, so it can be especially useful to outside audiences. I focused on MCC score because it takes all aspects of the confusion matrix into consideration; it is only high if the model is doing well classifying both negative and positive elements. I think for the context of the problem, it's important that my model performs well all around. There's a large amount of both sarcastic and non-sarcastic news headlines available, and I believe there should be emphasis on both the positive and negative classes. It's just as important to distiniguish a sarcastic headline from serious headlines as it is to distinguish a serious headline from sarcastic headlines. 

##### Discussion: 
I think this chosen procedure fits very well with the task at hand. It was a task involving NLP, which BERT was designed for, as well as binary classification, which the performance metrics are also suited for.

I think this type of model has the potential to help people better interpret the barrage of information they're presented with every day. As many people probably know, it can be hard to be on the same page as others when it comes to sarcasm, especially over text. While misinterpreting a friend's sarcastic comment might be harmless, reading a sarcastic news piece and truly thinking it's serious could have worse consequences. The same goes for reading a serious news piece and thinking it's satirical. Misinterpretation could just as quickly lead to the spread of misinformation. For instance, reading the news headline "NAACP demands less minority representation on UPN" as serious could lead to a very different interpretation than if it was read (correctly) as sarcasm. Therefore, a model like this could potentially be helpful if it were implemented on news sites to put a "sarcasm flag" next to articles that are meant as satire. 
This type of model could have other applications. It could be used to help people with ASD (Autism Spectrum Disorder) understand sarcasm, since people with ASD more often take sarcasm literally. This could help them communicate better with others. A model like this could be used on other platforms like X (Twitter), where a lot of misinformation is spread, to flag sarcastic tweets. 
A limitation to my current model is that its training data is very limited. While there are a lot of headlines, they only come from two news outlets: The Onion for sarcastic and HuffPost for non-sarcastic. Ideally, a lot more sources would be used for training. Furthermore, it only recognizes sarcasm in self-contained text instances. It would likely struggle with recognizing sarcastic responses to other text, since this is not self-containted. 

If I were to continue this project, I would try to expand the dataset to include headlines from other news outlets. I would also either build on this existing model or create a separate model that can identify sarcasm as a response to other text. This would be especially suitable for identifying sarcastic responses or re-tweets, for example. I could also try to train my model using different existing NLP architectures instead of BERT and see if performance improves.
A more ambitious idea would be to train a new model that can distinguish between sarcastic and non-sarcastic audio. This would involve collecting snippets of audio data and using a NLP architecture designed to parse audio instead of textual data. A tool like this could be integrated with existing voice services (Siri, Alexa) to enhance their communication with users. 

##### Citations:
1. Misra, Rishabh and Prahal Arora. "Sarcasm Detection using News Headlines Dataset." AI Open (2023).
2. Misra, Rishabh and Jigyasa Grover. "Sculpting Data for ML: The first act of Machine Learning." ISBN 9798585463570 (2021).
